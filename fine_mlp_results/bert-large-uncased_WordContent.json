{"0_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 0, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 18.53, "acc": 19.19}, "1_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 1, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 63.91, "acc": 64.55}, "2_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 2, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 64.9, "acc": 65.27}, "3_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 3, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 65.28, "acc": 65.71}, "4_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 4, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 66.47, "acc": 66.94}, "5_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 5, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 66.12, "acc": 66.03}, "6_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 6, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 65.02, "acc": 64.66}, "7_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 7, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 63.14, "acc": 62.26}, "8_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 8, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 62.15, "acc": 61.65}, "9_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 9, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 61.45, "acc": 60.66}, "10_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 10, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 59.05, "acc": 58.21}, "11_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 11, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 57.6, "acc": 56.91}, "12_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 12, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 55.44, "acc": 54.3}, "13_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 13, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 50.14, "acc": 49.71}, "14_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 14, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 46.04, "acc": 45.95}, "15_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 15, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 39.3, "acc": 39.03}, "16_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 16, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 36.51, "acc": 36.92}, "17_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 17, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 33.17, "acc": 33.44}, "18_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 18, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 31.27, "acc": 30.7}, "19_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 19, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 30.29, "acc": 30.0}, "20_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 20, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 29.69, "acc": 29.19}, "21_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 21, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 29.38, "acc": 29.18}, "22_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 22, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 28.18, "acc": 27.79}, "23_-1": {"device": ["1"], "batch_size": 500, "nhid": 50, "kfold": 10, "usepytorch": true, "data_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./fine_mlp_results/", "optim": "adam", "cbatch_size": 64, "tenacity": 5, "epoch_size": 4, "model_name": "bert-large-uncased", "task": "WordContent", "layer": 23, "head": -1, "head_size": 64, "seed": 123, "dropout": 0.0, "devacc": 28.02, "acc": 27.38}}