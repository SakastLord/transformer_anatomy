{"0_-1_fc": {"device": ["0"], "batch_size": 2000, "kfold": 5, "usepytorch": true, "task_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./linear_results/", "optim": "rmsprop", "cbatch_size": 256, "tenacity": 3, "epoch_size": 2, "model_name": "gpt2", "task": "WordContent", "layer": 0, "head": -1, "location": "fc", "head_size": 64, "dropout": 0, "nhid": 0, "seed": 123, "classifier": {"nhid": 0, "optim": "rmsprop", "tenacity": 3, "epoch_size": 2, "dropout": 0, "batch_size": 256}, "devacc": 18.62, "acc": 18.28}, "1_-1_fc": {"device": ["0"], "batch_size": 2000, "kfold": 5, "usepytorch": true, "task_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./linear_results/", "optim": "rmsprop", "cbatch_size": 256, "tenacity": 3, "epoch_size": 2, "model_name": "gpt2", "task": "WordContent", "layer": 1, "head": -1, "location": "fc", "head_size": 64, "dropout": 0, "nhid": 0, "seed": 123, "classifier": {"nhid": 0, "optim": "rmsprop", "tenacity": 3, "epoch_size": 2, "dropout": 0, "batch_size": 256}, "devacc": 11.35, "acc": 10.86}, "2_-1_fc": {"device": ["0"], "batch_size": 2000, "kfold": 5, "usepytorch": true, "task_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./linear_results/", "optim": "rmsprop", "cbatch_size": 256, "tenacity": 3, "epoch_size": 2, "model_name": "gpt2", "task": "WordContent", "layer": 2, "head": -1, "location": "fc", "head_size": 64, "dropout": 0, "nhid": 0, "seed": 123, "classifier": {"nhid": 0, "optim": "rmsprop", "tenacity": 3, "epoch_size": 2, "dropout": 0, "batch_size": 256}, "devacc": 14.31, "acc": 14.32}, "3_-1_fc": {"device": ["0"], "batch_size": 2000, "kfold": 5, "usepytorch": true, "task_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./linear_results/", "optim": "rmsprop", "cbatch_size": 256, "tenacity": 3, "epoch_size": 2, "model_name": "gpt2", "task": "WordContent", "layer": 3, "head": -1, "location": "fc", "head_size": 64, "dropout": 0, "nhid": 0, "seed": 123, "classifier": {"nhid": 0, "optim": "rmsprop", "tenacity": 3, "epoch_size": 2, "dropout": 0, "batch_size": 256}, "devacc": 14.31, "acc": 13.78}, "4_-1_fc": {"device": ["0"], "batch_size": 2000, "kfold": 5, "usepytorch": true, "task_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./linear_results/", "optim": "rmsprop", "cbatch_size": 256, "tenacity": 3, "epoch_size": 2, "model_name": "gpt2", "task": "WordContent", "layer": 4, "head": -1, "location": "fc", "head_size": 64, "dropout": 0, "nhid": 0, "seed": 123, "classifier": {"nhid": 0, "optim": "rmsprop", "tenacity": 3, "epoch_size": 2, "dropout": 0, "batch_size": 256}, "devacc": 15.56, "acc": 15.33}, "5_-1_fc": {"device": ["0"], "batch_size": 2000, "kfold": 5, "usepytorch": true, "task_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./linear_results/", "optim": "rmsprop", "cbatch_size": 256, "tenacity": 3, "epoch_size": 2, "model_name": "gpt2", "task": "WordContent", "layer": 5, "head": -1, "location": "fc", "head_size": 64, "dropout": 0, "nhid": 0, "seed": 123, "classifier": {"nhid": 0, "optim": "rmsprop", "tenacity": 3, "epoch_size": 2, "dropout": 0, "batch_size": 256}, "devacc": 16.13, "acc": 15.6}, "6_-1_fc": {"device": ["0"], "batch_size": 2000, "kfold": 5, "usepytorch": true, "task_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./linear_results/", "optim": "rmsprop", "cbatch_size": 256, "tenacity": 3, "epoch_size": 2, "model_name": "gpt2", "task": "WordContent", "layer": 6, "head": -1, "location": "fc", "head_size": 64, "dropout": 0, "nhid": 0, "seed": 123, "classifier": {"nhid": 0, "optim": "rmsprop", "tenacity": 3, "epoch_size": 2, "dropout": 0, "batch_size": 256}, "devacc": 17.65, "acc": 17.14}, "7_-1_fc": {"device": ["0"], "batch_size": 2000, "kfold": 5, "usepytorch": true, "task_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./linear_results/", "optim": "rmsprop", "cbatch_size": 256, "tenacity": 3, "epoch_size": 2, "model_name": "gpt2", "task": "WordContent", "layer": 7, "head": -1, "location": "fc", "head_size": 64, "dropout": 0, "nhid": 0, "seed": 123, "classifier": {"nhid": 0, "optim": "rmsprop", "tenacity": 3, "epoch_size": 2, "dropout": 0, "batch_size": 256}, "devacc": 17.94, "acc": 17.82}, "8_-1_fc": {"device": ["0"], "batch_size": 2000, "kfold": 5, "usepytorch": true, "task_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./linear_results/", "optim": "rmsprop", "cbatch_size": 256, "tenacity": 3, "epoch_size": 2, "model_name": "gpt2", "task": "WordContent", "layer": 8, "head": -1, "location": "fc", "head_size": 64, "dropout": 0, "nhid": 0, "seed": 123, "classifier": {"nhid": 0, "optim": "rmsprop", "tenacity": 3, "epoch_size": 2, "dropout": 0, "batch_size": 256}, "devacc": 19.39, "acc": 19.23}, "9_-1_fc": {"device": ["0"], "batch_size": 2000, "kfold": 5, "usepytorch": true, "task_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./linear_results/", "optim": "rmsprop", "cbatch_size": 256, "tenacity": 3, "epoch_size": 2, "model_name": "gpt2", "task": "WordContent", "layer": 9, "head": -1, "location": "fc", "head_size": 64, "dropout": 0, "nhid": 0, "seed": 123, "classifier": {"nhid": 0, "optim": "rmsprop", "tenacity": 3, "epoch_size": 2, "dropout": 0, "batch_size": 256}, "devacc": 23.12, "acc": 23.52}, "10_-1_fc": {"device": ["0"], "batch_size": 2000, "kfold": 5, "usepytorch": true, "task_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./linear_results/", "optim": "rmsprop", "cbatch_size": 256, "tenacity": 3, "epoch_size": 2, "model_name": "gpt2", "task": "WordContent", "layer": 10, "head": -1, "location": "fc", "head_size": 64, "dropout": 0, "nhid": 0, "seed": 123, "classifier": {"nhid": 0, "optim": "rmsprop", "tenacity": 3, "epoch_size": 2, "dropout": 0, "batch_size": 256}, "devacc": 26.07, "acc": 25.68}, "11_-1_fc": {"device": ["0"], "batch_size": 2000, "kfold": 5, "usepytorch": true, "task_path": "./SentEval/data/", "cache_path": "./cache/", "result_path": "./linear_results/", "optim": "rmsprop", "cbatch_size": 256, "tenacity": 3, "epoch_size": 2, "model_name": "gpt2", "task": "WordContent", "layer": 11, "head": -1, "location": "fc", "head_size": 64, "dropout": 0, "nhid": 0, "seed": 123, "classifier": {"nhid": 0, "optim": "rmsprop", "tenacity": 3, "epoch_size": 2, "dropout": 0, "batch_size": 256}, "devacc": 37.45, "acc": 37.47}}