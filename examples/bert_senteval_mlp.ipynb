{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import hashlib\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "\n",
    "PATH_SENTEVAL = './SentEval'\n",
    "PATH_TO_DATA = './SentEval/data/'\n",
    "PATH_TO_CACHE = './cache/'\n",
    "sys.path.insert(0, PATH_SENTEVAL)\n",
    "import senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentences_to_features(sentences, seq_length, tokenizer):\n",
    "    \"\"\"Convert sentence into Tensor\"\"\"\n",
    "    \n",
    "    num_sent = len(sentences)\n",
    "    input_type_ids = np.zeros((num_sent, seq_length), dtype=np.int32)\n",
    "    input_ids = np.zeros((num_sent, seq_length), dtype=np.int32)\n",
    "    input_mask = np.zeros((num_sent, seq_length), dtype=np.int32)\n",
    "    \n",
    "    for idx, sent in enumerate(sentences):\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        tokens = tokens[0:min((seq_length - 2), len(tokens))] # truncate tokens longer than seq_length\n",
    "        tokens.insert(0, \"[CLS]\")\n",
    "        tokens.append(\"[SEP]\")\n",
    "        \n",
    "        input_ids[idx,:len(tokens)] = np.array(tokenizer.convert_tokens_to_ids(tokens), dtype=np.int32)\n",
    "        input_mask[idx,:len(tokens)] = np.ones(len(tokens), dtype=np.int32)\n",
    "\n",
    "        assert len(input_ids[idx]) == seq_length\n",
    "        assert len(input_mask[idx]) == seq_length\n",
    "        assert len(input_type_ids[idx]) == seq_length\n",
    "\n",
    "    return input_ids, input_type_ids, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_exp_result(exp_result):\n",
    "    exp_key = '{}_{}'.format(exp_result['layer'], exp_result['head'])\n",
    "    print(exp_key)\n",
    "    result_name = \"{}_{}.json\".format(exp_result['model_name'], exp_result['task'])\n",
    "    result_dir = exp_result['result_path']\n",
    "    onlyfiles = [f for f in listdir(result_dir) if isfile(join(result_dir, f))]\n",
    "    if result_name in onlyfiles:\n",
    "        \n",
    "        with open(join(result_dir, result_name), 'r') as f:\n",
    "            results = json.load(f)\n",
    "            \n",
    "        with open(join(result_dir, result_name), 'w') as f:\n",
    "            results[exp_key] = exp_result\n",
    "            json.dump(results, f)\n",
    "        print(\"Append exp result at {} with key {}\".format(result_name, exp_key))\n",
    "            \n",
    "    else:\n",
    "        results = {}\n",
    "        with open(join(result_dir, result_name), 'w') as f:\n",
    "            results[exp_key] = exp_result\n",
    "            json.dump(results, f)\n",
    "        print(\"Create new exp result at {} with key {}\".format(result_name, exp_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache(model_name, task, cache_path):\n",
    "    cache_name = \"{}_{}.pickle\".format(model_name, task)\n",
    "    cache_dir = cache_path\n",
    "    onlyfiles = [f for f in listdir(cache_dir) if isfile(join(cache_dir, f))]\n",
    "\n",
    "    # ====== Look Up existing cache ====== #\n",
    "    if cache_name in onlyfiles:\n",
    "        print(\"cache Found {}\".format(cache_name))\n",
    "        with open(join(cache_dir, cache_name), 'rb') as f:\n",
    "            cache = pickle.load(f)\n",
    "            print(\"cache Loaded\")\n",
    "            return cache\n",
    "        \n",
    "    else:\n",
    "        print(\"cache not Found {}\".format(cache_name))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_batcher(batch):\n",
    "    max_capacity = 3000\n",
    "    seq_length = max([len(tokens) for tokens in batch])\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    mini_batch = max_capacity // seq_length + 1\n",
    "    return mini_batch\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    \n",
    "    if params['cache'] is None: # check whether cache is already provided\n",
    "        params['cache'] = load_cache(params.model_name, params.current_task, params.cache_path) # try to load cache\n",
    "\n",
    "        if params['cache'] is None: # if there is no cache saved, then construct encoder model\n",
    "            print(\"Constructing Encoder Model\")\n",
    "            params['cache'] = {}\n",
    "\n",
    "            # ====== Construct Model ====== #\n",
    "            model = BertModel.from_pretrained(args.model_name)\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            tokenizer = BertTokenizer.from_pretrained(args.model_name, do_lower_case=True)\n",
    "\n",
    "            params['model'] = model\n",
    "            params['tokenizer'] = tokenizer\n",
    "            params['flag_save'] = True\n",
    "             \n",
    "    # ====== Initializ Counter ====== #\n",
    "    params['count'] = 0\n",
    "\n",
    "\n",
    "def batcher(params, batch):\n",
    "    ts = time.time()\n",
    "    if params.cache != {}:\n",
    "        output = []\n",
    "        sentences = [' '.join(s) for s in batch]\n",
    "        for i, sent in enumerate(sentences):\n",
    "            hask_key = hashlib.sha256(sent.encode()).hexdigest()\n",
    "            output.append(params.cache[hask_key])\n",
    "        output = np.array(output)\n",
    "        \n",
    "    else:\n",
    "        mini_batch_size = efficient_batcher(batch)\n",
    "\n",
    "        idx = 0\n",
    "        list_output = []\n",
    "        while idx < len(batch):\n",
    "            mini_batch = batch[idx:min(idx+mini_batch_size, len(batch))]\n",
    "\n",
    "            # ====== Token Preparation ====== #\n",
    "            params.model.eval()\n",
    "            seq_length = max([len(tokens) for tokens in mini_batch])\n",
    "            sentences = [' '.join(s) for s in mini_batch]\n",
    "\n",
    "            # ====== Convert to Tensor ====== #\n",
    "            input_ids, input_type_ids, input_mask = convert_sentences_to_features(sentences, seq_length, params.tokenizer)\n",
    "            input_ids = torch.Tensor(input_ids).long().cuda()\n",
    "            input_type_ids = torch.Tensor(input_type_ids).long().cuda()\n",
    "            input_mask = torch.Tensor(input_mask).long().cuda()\n",
    "\n",
    "            # ====== Encode Tokens ====== #\n",
    "            encoded_layers, _ = model(input_ids, input_type_ids, input_mask)   \n",
    "            torch.cuda.synchronize()\n",
    "            output = np.array([layer[:, 0, :].detach().cpu().numpy() for layer in encoded_layers]) \n",
    "            output = np.swapaxes(output, 0, 1)\n",
    "            list_output.append(output)\n",
    "            idx += mini_batch_size\n",
    "\n",
    "            # ====== Construct Cache ====== #\n",
    "            temp_cache = {}\n",
    "            for i, sent in enumerate(sentences):\n",
    "                hask_key = hashlib.sha256(sent.encode()).hexdigest()\n",
    "                temp_cache[hask_key] = output[i]\n",
    "            params.cache.update(temp_cache)    \n",
    "            output = np.concatenate(list_output, 0)      \n",
    "    \n",
    "    te = time.time()\n",
    "    params.count += len(batch)\n",
    "    # ====== Extract Target Embedding (layer, head) ====== #\n",
    "    if params.head == -1:\n",
    "        embedding = output[:, params.layer, :]\n",
    "    else:\n",
    "        embedding = output[:, params.layer, params.head*params.head_size:(params.head+1)*params.head_size]\n",
    "    \n",
    "    if params.count % 20000 == 0:\n",
    "        print('{:6}'.format(params.count), 'encoded result', output.shape, 'return result', embedding.shape, 'took', '{:2.3f}'.format(te-ts), 'process', '{:4.1f}'.format(len(batch)/(te-ts)))\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(args, task, cache=None):\n",
    "    ts = time.time()\n",
    "        \n",
    "    # ====== SentEval Engine Setting ====== #\n",
    "    params_senteval = {'task_path': args.data_path, \n",
    "                       'seed': args.seed,\n",
    "                       'usepytorch': args.usepytorch, \n",
    "                       'kfold': args.kfold,\n",
    "                       'batch_size': args.batch_size}\n",
    "    \n",
    "    params_senteval['classifier'] = {'nhid': args.nhid, \n",
    "                                     'optim': args.optim, \n",
    "                                     'tenacity': args.tenacity,\n",
    "                                     'epoch_size': args.epoch_size,\n",
    "                                     'dropout': args.dropout,\n",
    "                                     'batch_size': args.cbatch_size,}\n",
    "    \n",
    "    # ====== Experiment Setting ====== #\n",
    "    params_senteval['model_name'] = args.model_name\n",
    "    params_senteval['cache_path'] = args.cache_path\n",
    "    params_senteval['result_path'] = args.result_path\n",
    "    params_senteval['layer'] = args.layer\n",
    "    params_senteval['head'] = args.head\n",
    "    params_senteval['head_size'] = args.head_size\n",
    "    params_senteval['cache'] = cache\n",
    "\n",
    "    # ====== Conduct Experiment ====== #\n",
    "    se = senteval.engine.SE(params_senteval, batcher, prepare)\n",
    "    result = se.eval([task])\n",
    "    \n",
    "    # ====== Logging Experiment Result ====== #\n",
    "    exp_result = vars(deepcopy(args))\n",
    "    exp_result['task'] = task\n",
    "    exp_result['devacc'] = result[task]['devacc']\n",
    "    exp_result['acc'] = result[task]['acc']\n",
    "    save_exp_result(exp_result)\n",
    "\n",
    "    # ====== Save Cache ====== #\n",
    "    if 'flag_save' in se.params:\n",
    "        print(\"Start saving cache\")\n",
    "        cache_name = \"{}_{}.pickle\".format(se.params.model_name, se.params.current_task)\n",
    "        cache_dir = se.params.cache_path\n",
    "        with open(join(cache_dir, cache_name), 'wb') as f:\n",
    "            pickle.dump(se.params.cache, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Saved cache {}\".format(cache_name))\n",
    "        \n",
    "    # ====== Reporting ====== #\n",
    "    te = time.time()\n",
    "    print(\"result: {}, took: {:3.1f} sec\".format(result, te-ts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Benchmark Configuration ======\n",
      "Device:  [1, 2]\n",
      "model name:  bert-base-uncased\n",
      "Task:  Length\n",
      "range layer:  range(0, 12)\n",
      "Total Exps:  108\n",
      "======================================\n",
      "  0%|          | 0/108 [00:00<?, ?it/s]\n",
      "P: 1:   0%|          | 0/108 [00:00<?, ?it/s]\n",
      "P: 2:   1%|          | 1/108 [00:00<00:00, 648.07it/s]\n",
      "P: 3:   2%|▏         | 2/108 [00:00<00:00, 786.04it/s]\n",
      "P: 4:   3%|▎         | 3/108 [00:00<00:00, 438.67it/s]\n",
      "P: 5:   4%|▎         | 4/108 [00:00<00:00, 509.08it/s]\n",
      "P: 6:   5%|▍         | 5/108 [00:00<00:00, 562.01it/s]\n",
      "P: 7:   6%|▌         | 6/108 [00:00<00:00, 602.85it/s]\n",
      "P: 8:   6%|▋         | 7/108 [00:00<00:00, 559.23it/s]\n",
      "P: 9:   7%|▋         | 8/108 [00:00<00:00, 539.84it/s]\n",
      "P: 10:   8%|▊         | 9/108 [00:00<00:00, 569.62it/s]\n",
      "P: 11:   9%|▉         | 10/108 [00:00<00:00, 557.77it/s]\n",
      "P: 12:  10%|█         | 11/108 [00:00<00:00, 561.15it/s]\n",
      "P: 13:  11%|█         | 12/108 [00:00<00:00, 583.87it/s]\n",
      "P: 14:  12%|█▏        | 13/108 [00:00<00:00, 596.56it/s]\n",
      "P: 15:  13%|█▎        | 14/108 [00:00<00:00, 616.40it/s]\n",
      "P: 16:  14%|█▍        | 15/108 [00:00<00:00, 582.30it/s]\n",
      "P: 17:  15%|█▍        | 16/108 [00:00<00:00, 597.41it/s]\n",
      "P: 18:  16%|█▌        | 17/108 [00:00<00:00, 589.06it/s]\n",
      "P: 19:  17%|█▋        | 18/108 [00:00<00:00, 590.42it/s]\n",
      "P: 20:  18%|█▊        | 19/108 [00:00<00:00, 603.70it/s]\n",
      "P: 21:  19%|█▊        | 20/108 [00:00<00:00, 617.24it/s]\n",
      "P: 22:  19%|█▉        | 21/108 [00:00<00:00, 599.34it/s]\n",
      "P: 23:  20%|██        | 22/108 [00:00<00:00, 611.03it/s]\n",
      "P: 24:  21%|██▏       | 23/108 [00:00<00:00, 601.67it/s]\n",
      "P: 25:  22%|██▏       | 24/108 [00:00<00:00, 612.15it/s]\n",
      "P: 26:  23%|██▎       | 25/108 [00:00<00:00, 623.33it/s]\n",
      "P: 27:  24%|██▍       | 26/108 [00:00<00:00, 611.58it/s]\n",
      "P: 28:  25%|██▌       | 27/108 [00:00<00:00, 608.83it/s]\n",
      "P: 29:  26%|██▌       | 28/108 [00:00<00:00, 618.59it/s]\n",
      "P: 30:  27%|██▋       | 29/108 [00:00<00:00, 611.29it/s]\n",
      "P: 31:  28%|██▊       | 30/108 [00:00<00:00, 618.98it/s]\n",
      "P: 32:  29%|██▊       | 31/108 [00:00<00:00, 614.10it/s]\n",
      "P: 33:  30%|██▉       | 32/108 [00:00<00:00, 622.60it/s]\n",
      "P: 34:  31%|███       | 33/108 [00:00<00:00, 625.60it/s]\n",
      "P: 35:  31%|███▏      | 34/108 [00:00<00:00, 618.65it/s]\n",
      "P: 36:  32%|███▏      | 35/108 [00:00<00:00, 625.87it/s]\n",
      "P: 37:  33%|███▎      | 36/108 [00:00<00:00, 619.96it/s]\n",
      "P: 38:  34%|███▍      | 37/108 [00:00<00:00, 623.40it/s]\n",
      "P: 39:  35%|███▌      | 38/108 [00:00<00:00, 618.18it/s]\n",
      "P: 40:  36%|███▌      | 39/108 [00:00<00:00, 624.24it/s]\n",
      "P: 41:  37%|███▋      | 40/108 [00:00<00:00, 629.31it/s]\n",
      "P: 42:  38%|███▊      | 41/108 [00:00<00:00, 635.78it/s]\n",
      "P: 43:  39%|███▉      | 42/108 [00:00<00:00, 641.30it/s]\n",
      "P: 44:  40%|███▉      | 43/108 [00:00<00:00, 646.60it/s]\n",
      "P: 45:  41%|████      | 44/108 [00:00<00:00, 652.05it/s]\n",
      "P: 46:  42%|████▏     | 45/108 [00:00<00:00, 657.23it/s]\n",
      "P: 47:  43%|████▎     | 46/108 [00:00<00:00, 663.49it/s]\n",
      "P: 48:  44%|████▎     | 47/108 [00:00<00:00, 625.25it/s]\n",
      "P: 49:  44%|████▍     | 48/108 [00:00<00:00, 630.83it/s]\n",
      "P: 50:  45%|████▌     | 49/108 [00:00<00:00, 625.92it/s]\n",
      "P: 51:  46%|████▋     | 50/108 [00:00<00:00, 631.33it/s]\n",
      "P: 52:  47%|████▋     | 51/108 [00:00<00:00, 627.15it/s]\n",
      "P: 53:  48%|████▊     | 52/108 [00:00<00:00, 632.19it/s]\n",
      "P: 54:  49%|████▉     | 53/108 [00:00<00:00, 635.79it/s]\n",
      "P: 55:  50%|█████     | 54/108 [00:00<00:00, 628.32it/s]\n",
      "P: 56:  51%|█████     | 55/108 [00:00<00:00, 632.92it/s]\n",
      "P: 57:  52%|█████▏    | 56/108 [00:00<00:00, 629.38it/s]\n",
      "P: 58:  53%|█████▎    | 57/108 [00:00<00:00, 633.50it/s]\n",
      "P: 59:  54%|█████▎    | 58/108 [00:00<00:00, 637.67it/s]\n",
      "P: 60:  55%|█████▍    | 59/108 [00:00<00:00, 628.81it/s]\n",
      "P: 61:  56%|█████▌    | 60/108 [00:00<00:00, 633.45it/s]\n",
      "P: 62:  56%|█████▋    | 61/108 [00:00<00:00, 637.70it/s]\n",
      "P: 63:  57%|█████▋    | 62/108 [00:00<00:00, 630.72it/s]\n",
      "P: 64:  59%|█████▉    | 64/108 [00:00<00:00, 634.68it/s]\n",
      "P: 65:  59%|█████▉    | 64/108 [00:00<00:00, 634.68it/s]\n",
      "P: 66:  60%|██████    | 65/108 [00:00<00:00, 634.68it/s]\n",
      "P: 67:  61%|██████    | 66/108 [00:00<00:00, 634.68it/s]\n",
      "P: 68:  62%|██████▏   | 67/108 [00:00<00:00, 634.68it/s]\n",
      "P: 69:  63%|██████▎   | 68/108 [00:00<00:00, 634.68it/s]\n",
      "P: 70:  64%|██████▍   | 69/108 [00:00<00:00, 634.68it/s]\n",
      "P: 71:  65%|██████▍   | 70/108 [00:00<00:00, 634.68it/s]\n",
      "P: 72:  66%|██████▌   | 71/108 [00:00<00:00, 634.68it/s]\n",
      "P: 73:  67%|██████▋   | 72/108 [00:00<00:00, 634.68it/s]\n",
      "P: 74:  68%|██████▊   | 73/108 [00:00<00:00, 634.68it/s]\n",
      "P: 75:  69%|██████▊   | 74/108 [00:00<00:00, 634.68it/s]\n",
      "P: 76:  69%|██████▉   | 75/108 [00:00<00:00, 634.68it/s]\n",
      "P: 77:  70%|███████   | 76/108 [00:00<00:00, 634.68it/s]\n",
      "P: 78:  71%|███████▏  | 77/108 [00:00<00:00, 634.68it/s]\n",
      "P: 79:  72%|███████▏  | 78/108 [00:00<00:00, 634.68it/s]\n",
      "P: 80:  73%|███████▎  | 79/108 [00:00<00:00, 634.68it/s]\n",
      "P: 81:  74%|███████▍  | 80/108 [00:00<00:00, 634.68it/s]\n",
      "P: 82:  75%|███████▌  | 81/108 [00:00<00:00, 634.68it/s]\n",
      "P: 83:  76%|███████▌  | 82/108 [00:00<00:00, 634.68it/s]\n",
      "P: 84:  77%|███████▋  | 83/108 [00:00<00:00, 634.68it/s]\n",
      "P: 85:  78%|███████▊  | 84/108 [00:00<00:00, 634.68it/s]\n",
      "P: 86:  79%|███████▊  | 85/108 [00:00<00:00, 634.68it/s]\n",
      "P: 87:  80%|███████▉  | 86/108 [00:00<00:00, 634.68it/s]\n",
      "P: 88:  81%|████████  | 87/108 [00:00<00:00, 634.68it/s]\n",
      "P: 89:  81%|████████▏ | 88/108 [00:00<00:00, 634.68it/s]\n",
      "P: 90:  82%|████████▏ | 89/108 [00:00<00:00, 634.68it/s]\n",
      "P: 91:  83%|████████▎ | 90/108 [00:00<00:00, 634.68it/s]\n",
      "P: 92:  84%|████████▍ | 91/108 [00:00<00:00, 634.68it/s]\n",
      "P: 93:  85%|████████▌ | 92/108 [00:00<00:00, 634.68it/s]\n",
      "P: 94:  86%|████████▌ | 93/108 [00:00<00:00, 634.68it/s]\n",
      "P: 95:  87%|████████▋ | 94/108 [00:00<00:00, 634.68it/s]\n",
      "P: 96:  88%|████████▊ | 95/108 [00:00<00:00, 634.68it/s]\n",
      "P: 97:  89%|████████▉ | 96/108 [00:00<00:00, 634.68it/s]\n",
      "P: 98:  90%|████████▉ | 97/108 [00:00<00:00, 634.68it/s]\n",
      "P: 99:  91%|█████████ | 98/108 [00:00<00:00, 634.68it/s]\n",
      "P: 100:  92%|█████████▏| 99/108 [00:00<00:00, 634.68it/s]\n",
      "P: 101:  93%|█████████▎| 100/108 [00:00<00:00, 634.68it/s]\n",
      "P: 102:  94%|█████████▎| 101/108 [00:00<00:00, 634.68it/s]\n",
      "P: 103:  94%|█████████▍| 102/108 [00:00<00:00, 634.68it/s]\n",
      "P: 104:  95%|█████████▌| 103/108 [00:00<00:00, 634.68it/s]\n",
      "P: 105:  96%|█████████▋| 104/108 [00:00<00:00, 634.68it/s]\n",
      "P: 106:  97%|█████████▋| 105/108 [00:00<00:00, 634.68it/s]\n",
      "P: 107:  98%|█████████▊| 106/108 [00:00<00:00, 634.68it/s]\n",
      "P: 108: 100%|██████████| 108/108 [00:00<00:00, 654.49it/s]\n"
     ]
    }
   ],
   "source": [
    "tasks = ['Length', 'WordContent', 'Depth', 'TopConstituents',\n",
    "         'BigramShift', 'Tense', 'SubjNumber', 'ObjNumber',\n",
    "         'OddManOut', 'CoordinationInversion', 'CR', 'MR', \n",
    "         'MPQA', 'SUBJ', 'SST2', 'SST5', \n",
    "         'TREC', 'MRPC', 'SNLI', 'SICKEntailment', \n",
    "         'SICKRelatedness', 'STSBenchmark', 'ImageCaptionRetrieval', 'STS12',\n",
    "         'STS13', 'STS14', 'STS15', 'STS16',]\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Evaluate BERT')\n",
    "parser.add_argument(\"--device\", type=list, default=[1,2])\n",
    "parser.add_argument(\"--batch_size\", type=int, default=500)\n",
    "parser.add_argument(\"--nhid\", type=int, default=0)\n",
    "parser.add_argument(\"--kfold\", type=int, default=10)\n",
    "parser.add_argument(\"--usepytorch\", type=bool, default=True)\n",
    "parser.add_argument(\"--data_path\", type=str, default='./SentEval/data/')\n",
    "parser.add_argument(\"--cache_path\", type=str, default='./cache/')\n",
    "parser.add_argument(\"--result_path\", type=str, default='./results/')\n",
    "parser.add_argument(\"--optim\", type=str, default='adam')\n",
    "parser.add_argument(\"--cbatch_size\", type=int, default=64)\n",
    "parser.add_argument(\"--tenacity\", type=int, default=5)\n",
    "parser.add_argument(\"--epoch_size\", type=int, default=4)\n",
    "parser.add_argument(\"--model_name\", type=str, default='bert-base-uncased')\n",
    "\n",
    "parser.add_argument(\"--task\", type=int, default=0)\n",
    "parser.add_argument(\"--layer\", type=int, default=[0, 11])\n",
    "parser.add_argument(\"--head\", type=int, default=[-1])\n",
    "parser.add_argument(\"--head_size\", type=int, default=64)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(str(x) for x in args.device)\n",
    "\n",
    "\n",
    "list_layer = range(args.layer[0], args.layer[1]+1) if len(args.layer) > 1 else [args.layer[0]]\n",
    "list_dropout = [0.0, 0.1, 0.2]\n",
    "list_nhid = [50, 100, 200]\n",
    "num_exp = len(list(list_layer)) * len(list_dropout) * len(list_nhid)\n",
    "\n",
    "\n",
    "print(\"======= Benchmark Configuration ======\")\n",
    "print(\"Device: \", args.device)\n",
    "print(\"model name: \", args.model_name)\n",
    "print(\"Task: \", tasks[args.task])\n",
    "print(\"range layer: \", list_layer)\n",
    "# print(\"range head: \", list_head)\n",
    "print(\"Total Exps: \", num_exp)\n",
    "print(\"======================================\")\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "loaded_cache = None\n",
    "target_task = tasks[args.task]\n",
    "\n",
    "with tqdm(total=num_exp, file=sys.stdout) as pbar:\n",
    "    for layer in list_layer:\n",
    "        for dropout in list_dropout:\n",
    "            for nhid in list_nhid:\n",
    "\n",
    "#                 if loaded_cache is None:\n",
    "#                     loaded_cache = load_cache(args.model_name, target_task, args.cache_path)          \n",
    "\n",
    "                args.layer = layer\n",
    "                args.dropout = dropout\n",
    "                args.nhid = nhid\n",
    "                print()\n",
    "#                 experiment(args, target_task, cache=loaded_cache)\n",
    "\n",
    "                pbar.set_description('P: %d' % (1 + cnt))\n",
    "                pbar.update(1)\n",
    "                cnt += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
