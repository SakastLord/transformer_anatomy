{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import hashlib\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "\n",
    "PATH_SENTEVAL = './SentEval'\n",
    "PATH_TO_DATA = './SentEval/data/'\n",
    "PATH_TO_CACHE = './cache/'\n",
    "sys.path.insert(0, PATH_SENTEVAL)\n",
    "import senteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentences_to_features(sentences, seq_length, tokenizer):\n",
    "    \"\"\"Convert sentence into Tensor\"\"\"\n",
    "    \n",
    "    num_sent = len(sentences)\n",
    "    input_type_ids = np.zeros((num_sent, seq_length), dtype=np.int32)\n",
    "    input_ids = np.zeros((num_sent, seq_length), dtype=np.int32)\n",
    "    input_mask = np.zeros((num_sent, seq_length), dtype=np.int32)\n",
    "    \n",
    "    for idx, sent in enumerate(sentences):\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        tokens = tokens[0:min((seq_length - 2), len(tokens))] # truncate tokens longer than seq_length\n",
    "        tokens.insert(0, \"[CLS]\")\n",
    "        tokens.append(\"[SEP]\")\n",
    "        \n",
    "        input_ids[idx,:len(tokens)] = np.array(tokenizer.convert_tokens_to_ids(tokens), dtype=np.int32)\n",
    "        input_mask[idx,:len(tokens)] = np.ones(len(tokens), dtype=np.int32)\n",
    "\n",
    "        assert len(input_ids[idx]) == seq_length\n",
    "        assert len(input_mask[idx]) == seq_length\n",
    "        assert len(input_type_ids[idx]) == seq_length\n",
    "\n",
    "    return input_ids, input_type_ids, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_exp_result(exp_result):\n",
    "    exp_key = '{}_{}'.format(exp_result['layer'], exp_result['head'])\n",
    "    print(exp_key)\n",
    "    result_name = \"{}_{}.json\".format(exp_result['model_name'], exp_result['task'])\n",
    "    result_dir = exp_result['result_path']\n",
    "    onlyfiles = [f for f in listdir(result_dir) if isfile(join(result_dir, f))]\n",
    "    if result_name in onlyfiles:\n",
    "        \n",
    "        with open(join(result_dir, result_name), 'r') as f:\n",
    "            results = json.load(f)\n",
    "            \n",
    "        with open(join(result_dir, result_name), 'w') as f:\n",
    "            results[exp_key] = exp_result\n",
    "            json.dump(results, f)\n",
    "        print(\"Append exp result at {} with key {}\".format(result_name, exp_key))\n",
    "            \n",
    "    else:\n",
    "        results = {}\n",
    "        with open(join(result_dir, result_name), 'w') as f:\n",
    "            results[exp_key] = exp_result\n",
    "            json.dump(results, f)\n",
    "        print(\"Create new exp result at {} with key {}\".format(result_name, exp_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache(model_name, task, cache_path):\n",
    "    cache_name = \"{}_{}.pickle\".format(model_name, task)\n",
    "    cache_dir = cache_path\n",
    "    onlyfiles = [f for f in listdir(cache_dir) if isfile(join(cache_dir, f))]\n",
    "\n",
    "    # ====== Look Up existing cache ====== #\n",
    "    if cache_name in onlyfiles:\n",
    "        print(\"cache Found {}\".format(cache_name))\n",
    "        with open(join(cache_dir, cache_name), 'rb') as f:\n",
    "            cache = pickle.load(f)\n",
    "            print(\"cache Loaded\")\n",
    "            return cache\n",
    "        \n",
    "    else:\n",
    "        print(\"cache not Found {}\".format(cache_name))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_batcher(batch):\n",
    "    max_capacity = 3000\n",
    "    seq_length = max([len(tokens) for tokens in batch])\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    mini_batch = max_capacity // seq_length + 1\n",
    "    return mini_batch\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    \n",
    "    if params['cache'] is None: # check whether cache is already provided\n",
    "        params['cache'] = load_cache(params.model_name, params.current_task, params.cache_path) # try to load cache\n",
    "\n",
    "        if params['cache'] is None: # if there is no cache saved, then construct encoder model\n",
    "            print(\"Constructing Encoder Model\")\n",
    "            params['cache'] = {}\n",
    "\n",
    "            # ====== Construct Model ====== #\n",
    "            model = BertModel.from_pretrained(args.model_name)\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            tokenizer = BertTokenizer.from_pretrained(args.model_name, do_lower_case=True)\n",
    "\n",
    "            params['model'] = model\n",
    "            params['tokenizer'] = tokenizer\n",
    "            params['flag_save'] = True\n",
    "             \n",
    "    # ====== Initializ Counter ====== #\n",
    "    params['count'] = 0\n",
    "\n",
    "\n",
    "def batcher(params, batch):\n",
    "    ts = time.time()\n",
    "    if params.cache != {}:\n",
    "        output = []\n",
    "        sentences = [' '.join(s) for s in batch]\n",
    "        for i, sent in enumerate(sentences):\n",
    "            hask_key = hashlib.sha256(sent.encode()).hexdigest()\n",
    "            output.append(params.cache[hask_key])\n",
    "        output = np.array(output)\n",
    "        \n",
    "    else:\n",
    "        mini_batch_size = efficient_batcher(batch)\n",
    "\n",
    "        idx = 0\n",
    "        list_output = []\n",
    "        while idx < len(batch):\n",
    "            mini_batch = batch[idx:min(idx+mini_batch_size, len(batch))]\n",
    "\n",
    "            # ====== Token Preparation ====== #\n",
    "            params.model.eval()\n",
    "            seq_length = max([len(tokens) for tokens in mini_batch])\n",
    "            sentences = [' '.join(s) for s in mini_batch]\n",
    "\n",
    "            # ====== Convert to Tensor ====== #\n",
    "            input_ids, input_type_ids, input_mask = convert_sentences_to_features(sentences, seq_length, params.tokenizer)\n",
    "            input_ids = torch.Tensor(input_ids).long().cuda()\n",
    "            input_type_ids = torch.Tensor(input_type_ids).long().cuda()\n",
    "            input_mask = torch.Tensor(input_mask).long().cuda()\n",
    "\n",
    "            # ====== Encode Tokens ====== #\n",
    "            encoded_layers, _ = model(input_ids, input_type_ids, input_mask)   \n",
    "            torch.cuda.synchronize()\n",
    "            output = np.array([layer[:, 0, :].detach().cpu().numpy() for layer in encoded_layers]) \n",
    "            output = np.swapaxes(output, 0, 1)\n",
    "            list_output.append(output)\n",
    "            idx += mini_batch_size\n",
    "\n",
    "            # ====== Construct Cache ====== #\n",
    "            temp_cache = {}\n",
    "            for i, sent in enumerate(sentences):\n",
    "                hask_key = hashlib.sha256(sent.encode()).hexdigest()\n",
    "                temp_cache[hask_key] = output[i]\n",
    "            params.cache.update(temp_cache)    \n",
    "            output = np.concatenate(list_output, 0)      \n",
    "    \n",
    "    te = time.time()\n",
    "    params.count += len(batch)\n",
    "    # ====== Extract Target Embedding (layer, head) ====== #\n",
    "    if params.head == -1:\n",
    "        embedding = output[:, params.layer, :]\n",
    "    else:\n",
    "        embedding = output[:, params.layer, params.head*params.head_size:(params.head+1)*params.head_size]\n",
    "    \n",
    "    if params.count % 20000 == 0:\n",
    "        print('{:6}'.format(params.count), 'encoded result', output.shape, 'return result', embedding.shape, 'took', '{:2.3f}'.format(te-ts), 'process', '{:4.1f}'.format(len(batch)/(te-ts)))\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(args, task, cache=None):\n",
    "    ts = time.time()\n",
    "        \n",
    "    # ====== SentEval Engine Setting ====== #\n",
    "    params_senteval = {'task_path': args.data_path, \n",
    "                       'seed': args.seed,\n",
    "                       'usepytorch': args.usepytorch, \n",
    "                       'kfold': args.kfold,\n",
    "                       'batch_size': args.batch_size}\n",
    "    \n",
    "    params_senteval['classifier'] = {'nhid': args.nhid, \n",
    "                                     'optim': args.optim, \n",
    "                                     'tenacity': args.tenacity,\n",
    "                                     'epoch_size': args.epoch_size,\n",
    "                                     'dropout': args.dropout,\n",
    "                                     'batch_size': args.cbatch_size,}\n",
    "    \n",
    "    # ====== Experiment Setting ====== #\n",
    "    params_senteval['model_name'] = args.model_name\n",
    "    params_senteval['cache_path'] = args.cache_path\n",
    "    params_senteval['result_path'] = args.result_path\n",
    "    params_senteval['layer'] = args.layer\n",
    "    params_senteval['head'] = args.head\n",
    "    params_senteval['head_size'] = args.head_size\n",
    "    params_senteval['cache'] = cache\n",
    "\n",
    "    # ====== Conduct Experiment ====== #\n",
    "    se = senteval.engine.SE(params_senteval, batcher, prepare)\n",
    "    result = se.eval([task])\n",
    "    \n",
    "    # ====== Logging Experiment Result ====== #\n",
    "    exp_result = vars(deepcopy(args))\n",
    "    exp_result['task'] = task\n",
    "    exp_result['devacc'] = result[task]['devacc']\n",
    "    exp_result['acc'] = result[task]['acc']\n",
    "    save_exp_result(exp_result)\n",
    "\n",
    "    # ====== Save Cache ====== #\n",
    "    if 'flag_save' in se.params:\n",
    "        print(\"Start saving cache\")\n",
    "        cache_name = \"{}_{}.pickle\".format(se.params.model_name, se.params.current_task)\n",
    "        cache_dir = se.params.cache_path\n",
    "        with open(join(cache_dir, cache_name), 'wb') as f:\n",
    "            pickle.dump(se.params.cache, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Saved cache {}\".format(cache_name))\n",
    "        \n",
    "    # ====== Reporting ====== #\n",
    "    te = time.time()\n",
    "    print(\"result: {}, took: {:3.1f} sec\".format(result, te-ts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['Length', 'WordContent', 'Depth', 'TopConstituents',\n",
    "         'BigramShift', 'Tense', 'SubjNumber', 'ObjNumber',\n",
    "         'OddManOut', 'CoordinationInversion', 'CR', 'MR', \n",
    "         'MPQA', 'SUBJ', 'SST2', 'SST5', \n",
    "         'TREC', 'MRPC', 'SNLI', 'SICKEntailment', \n",
    "         'SICKRelatedness', 'STSBenchmark', 'ImageCaptionRetrieval', 'STS12',\n",
    "         'STS13', 'STS14', 'STS15', 'STS16',]\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Evaluate BERT')\n",
    "parser.add_argument(\"--device\", type=list, default=[1,2])\n",
    "parser.add_argument(\"--batch_size\", type=int, default=500)\n",
    "parser.add_argument(\"--nhid\", type=int, default=0)\n",
    "parser.add_argument(\"--kfold\", type=int, default=5)\n",
    "parser.add_argument(\"--usepytorch\", type=bool, default=True)\n",
    "parser.add_argument(\"--data_path\", type=str, default='./SentEval/data/')\n",
    "parser.add_argument(\"--cache_path\", type=str, default='./cache/')\n",
    "parser.add_argument(\"--result_path\", type=str, default='./results/')\n",
    "parser.add_argument(\"--optim\", type=str, default='rmsprop')\n",
    "parser.add_argument(\"--cbatch_size\", type=int, default=64)\n",
    "parser.add_argument(\"--tenacity\", type=int, default=5)\n",
    "parser.add_argument(\"--epoch_size\", type=int, default=4)\n",
    "parser.add_argument(\"--model_name\", type=str, default='bert-large-uncased')\n",
    "\n",
    "parser.add_argument(\"--task\", type=int, default=0)\n",
    "parser.add_argument(\"--layer\", type=int, default=[0])\n",
    "parser.add_argument(\"--head\", type=int, default=[-1, 11])\n",
    "parser.add_argument(\"--head_size\", type=int, default=64)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(str(x) for x in args.device)\n",
    "\n",
    "\n",
    "list_layer = range(args.layer[0], args.layer[1]+1) if len(args.layer) > 1 else [args.layer[0]]\n",
    "list_head = range(args.head[0], args.head[1]+1) if len(args.head) > 1 else [args.head[0]]\n",
    "num_exp = len(list(list_layer)) * len(list(list_head))\n",
    "\n",
    "\n",
    "print(\"======= Benchmark Configuration ======\")\n",
    "print(\"Device: \", args.device)\n",
    "print(\"model name: \", args.model_name)\n",
    "print(\"Task: \", tasks[args.task])\n",
    "print(\"range layer: \", list_layer)\n",
    "print(\"range head: \", list_head)\n",
    "print(\"Total Exps: \", num_exp)\n",
    "print(\"======================================\")\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "loaded_cache = None\n",
    "target_task = tasks[args.task]\n",
    "\n",
    "with tqdm(total=num_exp, file=sys.stdout) as pbar:\n",
    "    for layer in list_layer:\n",
    "        for head in list_head:\n",
    "\n",
    "            if loaded_cache is None:\n",
    "                loaded_cache = load_cache(args.model_name, target_task, args.cache_path)          \n",
    "\n",
    "            args.layer = layer\n",
    "            args.head = head\n",
    "            print()\n",
    "            experiment(args, target_task, cache=loaded_cache)\n",
    "\n",
    "            pbar.set_description('P: %d' % (1 + cnt))\n",
    "            pbar.update(1)\n",
    "            cnt += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
